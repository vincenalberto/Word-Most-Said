{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <font color='gold'> DetikOto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "from requests.exceptions import ConnectionError\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='pink'> Dapetin Link Berita dan tanggalnya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllBerita(self, details, page, cat_link, category, date=datetime.strftime(datetime.today(), '%m/%d/%Y')):\n",
    "    \"\"\"\n",
    "    Untuk mengambil seluruh url\n",
    "    link pada indeks category tertentu\n",
    "    date format : dd/mm/YYYY\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"page \", page)\n",
    "    if cat_link == 'news':\n",
    "        url = \"https://\"+cat_link+\".detik.com/indeks/all/\"+str(page)+\"?date=\"+date\n",
    "    else :\n",
    "        url = \"https://\"+cat_link+\".detik.com/indeks/\"+str(page)+\"?date=\"+date\n",
    "    print(url)\n",
    "    # Make the request and create the response object: response\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "    except ConnectionError:\n",
    "        print(\"Connection Error, but it's still trying...\")\n",
    "        time.sleep(10)\n",
    "        details = self.getAllBerita(details, page, cat_link, category, date)\n",
    "    # Extract HTML texts contained in Response object: html\n",
    "    html = response.text\n",
    "    # Create a BeautifulSoup object from the HTML: soup\n",
    "    soup = BeautifulSoup(html, \"html5lib\")\n",
    "    contentDiv = soup.find('div', attrs={'class':'lf_content'})\n",
    "    if not contentDiv:\n",
    "        contentDiv = soup.find('div', attrs={'class':'content right'})\n",
    "        if not contentDiv:\n",
    "            contentDiv = soup.find('div', attrs={'class':'rm_content'})\n",
    "    indeks = contentDiv.findAll('article')\n",
    "    for post in indeks:\n",
    "        cek_foto = post.find('span', {'class':'sub_judul'})\n",
    "        if cek_foto:\n",
    "            print(cek_foto.get_text(strip=True).lower())\n",
    "            if (\"foto\" in cek_foto.get_text(strip=True).lower()) or (\"video\" in cek_foto.get_text(strip=True).lower()) or (\"fotoinet\" in cek_foto.get_text(strip=True).lower()) or (\"videoinet\" in cek_foto.get_text(strip=True).lower()):\n",
    "                continue\n",
    "        link = [post.find('a', href=True)['href'], category]\n",
    "        detail = self.getDetailBerita(0, link)\n",
    "        if detail:\n",
    "            if self.insertDB(detail):\n",
    "                details.append(detail)\n",
    "\n",
    "    el_page = soup.find('div', class_=\"paging paging2\")\n",
    "    if el_page:\n",
    "        max_page = int(soup.find('div', class_=\"paging paging2\").findAll('a')[-2].get_text(strip=True).replace('\\n', '').strip(' '))\n",
    "\n",
    "        if page < max_page:\n",
    "            time.sleep(10)\n",
    "            details = self.getAllBerita(details, page+1, cat_link, category, date)\n",
    "\n",
    "    return 'berhasil ambil semua berita'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='pink'> Dapetin isi berita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDetailBerita(self, count, link):\n",
    "    \"\"\"\n",
    "    Mengambil seluruh element dari halaman berita\n",
    "    \"\"\"\n",
    "    time.sleep(10)\n",
    "    articles = {}\n",
    "    #link\n",
    "    url = link[0]\n",
    "\n",
    "    print(url)\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "    except:\n",
    "        return False\n",
    "    html = response.text\n",
    "    # Create a BeautifulSoup object from the HTML: soup\n",
    "    soup = BeautifulSoup(html, \"html5lib\")\n",
    "    # print(soup)\n",
    "    #extract subcategory from breadcrumb\n",
    "    bc = soup.find('div', class_=\"breadcrumb\")\n",
    "    if not bc:\n",
    "        return False\n",
    "\n",
    "    sub = bc.findAll('a')[1].get_text(strip=True)\n",
    "    if (\"foto\" in sub.lower()) or (\"detiktv\" in sub.lower()) or (\"video\" in sub.lower()) or (\"photos\" in sub.lower()) or (\"videos\" in sub.lower()):\n",
    "        return False\n",
    "\n",
    "    articles['subcategory'] = sub\n",
    "    #category\n",
    "    articles['category'] = link[1]\n",
    "    articles['url'] = url\n",
    "\n",
    "    article = soup.find('article')\n",
    "\n",
    "    #extract date\n",
    "    pubdate = soup.find(\"meta\", attrs={'name':'publishdate'})\n",
    "    if pubdate:\n",
    "        pubdate = pubdate['content'].strip(' \\t\\n\\r')\n",
    "        articles['pubdate'] = datetime.strftime(datetime.strptime(pubdate, \"%Y/%m/%d %H:%M:%S\"), '%Y-%m-%d %H:%M:%S')\n",
    "        id = soup.find(\"meta\", attrs={'name':'articleid'})\n",
    "        articles['id'] = int(id['content']) if id else int(datetime.strptime(pubdate, \"%Y/%m/%d %H:%M:%S\").timestamp()) + len(url)\n",
    "    else:\n",
    "        pubdate = soup.find('span', {'class':'date'})\n",
    "        pubdate = pubdate.get_text(strip=True).strip(' \\t\\n\\r').replace(\" WIB\", '')\n",
    "        articles['pubdate'] = datetime.strftime(datetime.strptime(pubdate, \"%A, %d %b %Y %H:%M\"), '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        id = soup.find(\"meta\", attrs={'name':'articleid'})\n",
    "        articles['id'] = int(id['content']) if id else int(datetime.strptime(pubdate, \"%A, %d %b %Y %H:%M\").timestamp()) + len(url)\n",
    "\n",
    "    #extract author\n",
    "    author = soup.find(\"meta\", attrs={'name':'author'})\n",
    "    articles['author'] = author['content'] if author else ''\n",
    "\n",
    "    #extract title\n",
    "    title =  article.find('meta', {\"property\":\"og:title\"})\n",
    "    articles['title'] = title.get_text(strip=True) if title else ''\n",
    "\n",
    "    #source\n",
    "    articles['source'] = 'detik'\n",
    "\n",
    "    #extract comments count\n",
    "    komentar = soup.find('a', class_=\"komentar\")\n",
    "    articles['comments'] = int(komentar.find('span').get_text(strip=True).replace('Komentar', '').strip(' \\t\\n\\r')) if komentar else 0\n",
    "\n",
    "    #extract tags\n",
    "    tags = article.find('div', class_=\"detail_tag\")\n",
    "    articles['tags'] = ','.join([x.get_text(strip=True) for x in tags.findAll('a')]) if tags else ''\n",
    "\n",
    "    #extract images\n",
    "    images = article.find('div', class_=\"pic_artikel\")\n",
    "    articles['images'] = images.find('img')['src'] if images else ''\n",
    "\n",
    "    #extract detail\n",
    "    if articles['category'] == 'news':\n",
    "        detail = article.find('div', class_=\"detail_text\")\n",
    "    else:\n",
    "        detail = article.find('div', attrs={\"id\": \"detikdetailtext\"})\n",
    "        if not detail:\n",
    "            detail = soup.find('div', attrs={\"class\": \"read__content full mt20\"})\n",
    "            if not detail:\n",
    "                detail = soup.find('div', attrs={\"id\": \"detikdetailtext\"})\n",
    "    if not detail:\n",
    "        return False\n",
    "    #hapus link sisip\n",
    "    if detail.findAll('table', class_=\"linksisip\"):\n",
    "        for link in detail.findAll('table', class_=\"linksisip\"):\n",
    "            link.decompose()\n",
    "\n",
    "    #hapus video sisip\n",
    "    if detail.findAll('div', class_=\"sisip_embed_sosmed\"):\n",
    "        for tag in detail.findAll('div', class_=\"sisip_embed_sosmed\"):\n",
    "            tag.decompose()\n",
    "\n",
    "    #hapus all setelah clear fix\n",
    "    if detail.find('div', class_=\"clearfix mb20\"):\n",
    "        for det in detail.find('div', class_=\"clearfix mb20\").findAllNext():\n",
    "            det.decompose()\n",
    "\n",
    "    #hapus all script\n",
    "    for script in detail.findAll('script'):\n",
    "        script.decompose()\n",
    "\n",
    "    for p in detail.findAll('p'):\n",
    "       if (\"baca juga\" in p.get_text(strip=True).lower()) and (p.find('a')):\n",
    "           p.decompose()\n",
    "\n",
    "    #extract content\n",
    "    detail = BeautifulSoup(detail.decode_contents().replace('<br/>', ' '), \"html5lib\")\n",
    "    content = re.sub(r'\\n|\\t|\\b|\\r','',unicodedata.normalize(\"NFKD\",detail.get_text(strip=True)))\n",
    "    articles['content'] = re.sub(r'(Tonton juga).*','', content)\n",
    "    print('memasukkan berita id ', articles['id'])\n",
    "\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='pink'> INSERT Artikel ke dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertDF(self, articles):\n",
    "    \"\"\"\n",
    "    Untuk memasukkan berita ke DB\n",
    "    \"\"\"\n",
    "    con = mysql.connector.connect(user='root', password='', host='127.0.0.1', database='news_db')\n",
    "    print(\"Insert berita \", articles['title'])\n",
    "    cursor = con.cursor()\n",
    "    query = \"SELECT count(*) FROM article WHERE url like '\"+articles['url']+\"'\"\n",
    "    cursor.execute(query)\n",
    "    result = cursor.fetchone()\n",
    "    if result[0] <= 0:\n",
    "        add_article = (\"INSERT INTO article (post_id, author, pubdate, category, subcategory, content, comments, images, title, tags, url, source) VALUES (%(id)s, %(author)s, %(pubdate)s, %(category)s, %(subcategory)s, %(content)s, %(comments)s, %(images)s, %(title)s, %(tags)s, %(url)s, %(source)s)\")\n",
    "        # Insert article\n",
    "        cursor.execute(add_article, articles)\n",
    "        con.commit()\n",
    "        print('masuk')\n",
    "        cursor.close()\n",
    "        con.close()\n",
    "        return True\n",
    "    else:\n",
    "        cursor.close()\n",
    "        print('salah2')\n",
    "        con.close()\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detik_post:\n",
    "    def getAllBerita(self, details, cat_link, category, date=datetime.strftime(datetime.today(), '%m/%d/%Y')):\n",
    "        \"\"\"\n",
    "        Untuk mengambil seluruh url\n",
    "        link pada indeks category tertentu\n",
    "        date format : dd/mm/YYYY\n",
    "        \"\"\"\n",
    "\n",
    "        # print(\"page \", page)\n",
    "        # s = requests.Session()\n",
    "        url = \"https://\"+cat_link+\".detik.com/indeks/\"\n",
    "        formdata = {}\n",
    "        formdata['datepick'] = date\n",
    "        # Make the request and create the response object: response\n",
    "        try:\n",
    "            response = requests.post(url, data=formdata)\n",
    "        except ConnectionError:\n",
    "            print(\"Connection Error, but it's still trying...\")\n",
    "            time.sleep(10)\n",
    "            details = self.getAllBerita(details, cat_link, category, date)\n",
    "        # Extract HTML texts contained in Response object: html\n",
    "        html = response.text\n",
    "        # Create a BeautifulSoup object from the HTML: soup\n",
    "        soup = BeautifulSoup(html, \"html5lib\")\n",
    "        contentDiv = soup.find('div', attrs={'class':'lf_content'})\n",
    "        if not contentDiv:\n",
    "            contentDiv = soup.find('div', attrs={'class':'content right'})\n",
    "            if not contentDiv:\n",
    "                contentDiv = soup.find('div', attrs={'class':'rm_content'})\n",
    "        indeks = contentDiv.findAll('article')\n",
    "        for post in indeks:\n",
    "            cek_foto = post.find('span', {'class':'sub_judul'})\n",
    "            if cek_foto:\n",
    "                print(cek_foto.get_text(strip=True).lower())\n",
    "                if (\"foto\" in cek_foto.get_text(strip=True).lower()) or (\"video\" in cek_foto.get_text(strip=True).lower()) or (\"fotoinet\" in cek_foto.get_text(strip=True).lower()) or (\"videoinet\" in cek_foto.get_text(strip=True).lower()):\n",
    "                    continue\n",
    "            link = [post.find('a', href=True)['href'], category]\n",
    "            detail = self.getDetailBerita(0, link)\n",
    "            if detail:\n",
    "                if self.insertDB(detail):\n",
    "                    details.append(detail)\n",
    "\n",
    "        el_page = soup.find('div', class_=\"paging paging2\")\n",
    "        if el_page:\n",
    "            max_page = int(soup.find('div', class_=\"paging paging2\").findAll('a')[-2].get_text(strip=True).replace('\\n', '').strip(' '))\n",
    "\n",
    "            if page < max_page:\n",
    "                time.sleep(10)\n",
    "                details = self.getAllBerita(details, cat_link, category, date)\n",
    "\n",
    "        return 'berhasil ambil semua berita'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
